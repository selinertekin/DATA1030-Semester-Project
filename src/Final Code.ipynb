{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1 - Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import partial_dependence\n",
    "from xgboost import XGBRegressor\n",
    "import shap\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Data Loading Function\n",
    "def load_and_clean_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean the air quality dataset with improved handling of missing values\n",
    "    and feature engineering.\n",
    "    \"\"\"\n",
    "    header = [\n",
    "        \"No\", \"year\", \"month\", \"day\", \"hour\", \"pm2.5\", \"DEWP\",\n",
    "        \"TEMP\", \"PRES\", \"cbwd\", \"Iws\", \"Is\", \"Ir\"\n",
    "    ]\n",
    "    df = pd.read_csv(file_path, names=header, delimiter=',', skiprows=1)\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    df = df[pd.to_numeric(df['year'], errors='coerce').notnull()]\n",
    "    numeric_cols = ['year', 'month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    print(\"\\nMissing values before cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Create datetime and additional temporal features\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "    df['season'] = df['month'].apply(lambda x: (x%12 + 3)//3)\n",
    "    df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=['pm2.5']).sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nMissing values after cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Feature Engineering Function:\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced features for the model\"\"\"\n",
    "    print(\"Creating advanced features...\")\n",
    "    \n",
    "    # Time-based features\n",
    "    df['hour_of_day'] = df['hour']\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month_of_year'] = df['month']\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    for col, max_val in [('hour_of_day', 24), ('day_of_week', 7), ('month_of_year', 12)]:\n",
    "        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col]/max_val)\n",
    "        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col]/max_val)\n",
    "    \n",
    "    # Weather interactions\n",
    "    df['humid_temp'] = df['TEMP'] * df['DEWP']\n",
    "    df['wind_pressure'] = df['PRES'] * df['Iws']\n",
    "    \n",
    "    # Rolling statistics\n",
    "    windows = [6, 12, 24, 48]\n",
    "    for col in ['TEMP', 'DEWP', 'PRES', 'Iws']:\n",
    "        for window in windows:\n",
    "            df[f'{col}_rolling_mean_{window}h'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'{col}_rolling_std_{window}h'] = df[col].rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # Lag features\n",
    "    lag_hours = [24, 48, 72]\n",
    "    for lag in lag_hours:\n",
    "        df[f'pm25_lag_{lag}h'] = df['pm2.5'].shift(lag)\n",
    "    \n",
    "    print(f\"Number of features after engineering: {len(df.columns)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Preprocessor Function\n",
    "\n",
    "def create_preprocessor():\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline for both numerical and categorical features.\n",
    "    \"\"\"\n",
    "    # Define numeric features based on actual columns after feature engineering\n",
    "    numeric_features = [\n",
    "        'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', \n",
    "        'season', 'is_weekend', 'hour_sin', 'hour_cos',\n",
    "        'hour_of_day', 'day_of_week', 'month_of_year',\n",
    "        'hour_of_day_sin', 'hour_of_day_cos',\n",
    "        'day_of_week_sin', 'day_of_week_cos',\n",
    "        'month_of_year_sin', 'month_of_year_cos',\n",
    "        'humid_temp', 'wind_pressure'\n",
    "    ]\n",
    "    \n",
    "    # Add rolling statistics features\n",
    "    for col in ['TEMP', 'DEWP', 'PRES', 'Iws']:\n",
    "        for window in [6, 12, 24, 48]:\n",
    "            numeric_features.append(f'{col}_rolling_mean_{window}h')\n",
    "            numeric_features.append(f'{col}_rolling_std_{window}h')\n",
    "    \n",
    "    # Add lag features\n",
    "    for lag in [24, 48, 72]:\n",
    "        numeric_features.append(f'pm25_lag_{lag}h')\n",
    "    \n",
    "    categorical_features = ['cbwd']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (43825, 13)\n",
      "\n",
      "Missing values before cleaning:\n",
      "No          0\n",
      "year        0\n",
      "month       0\n",
      "day         0\n",
      "hour        0\n",
      "pm2.5    2067\n",
      "DEWP        0\n",
      "TEMP        0\n",
      "PRES        0\n",
      "cbwd        0\n",
      "Iws         0\n",
      "Is          0\n",
      "Ir          0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "No            0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "hour          0\n",
      "pm2.5         0\n",
      "DEWP          0\n",
      "TEMP          0\n",
      "PRES          0\n",
      "cbwd          0\n",
      "Iws           0\n",
      "Is            0\n",
      "Ir            0\n",
      "date          0\n",
      "season        0\n",
      "is_weekend    0\n",
      "hour_sin      0\n",
      "hour_cos      0\n",
      "dtype: int64\n",
      "Final data shape: (41757, 18)\n",
      "Creating advanced features...\n",
      "Number of features after engineering: 64\n",
      "\n",
      "Columns in processed dataframe:\n",
      "['No', 'year', 'month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir', 'date', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'hour_of_day', 'day_of_week', 'month_of_year', 'hour_of_day_sin', 'hour_of_day_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_of_year_sin', 'month_of_year_cos', 'humid_temp', 'wind_pressure', 'TEMP_rolling_mean_6h', 'TEMP_rolling_std_6h', 'TEMP_rolling_mean_12h', 'TEMP_rolling_std_12h', 'TEMP_rolling_mean_24h', 'TEMP_rolling_std_24h', 'TEMP_rolling_mean_48h', 'TEMP_rolling_std_48h', 'DEWP_rolling_mean_6h', 'DEWP_rolling_std_6h', 'DEWP_rolling_mean_12h', 'DEWP_rolling_std_12h', 'DEWP_rolling_mean_24h', 'DEWP_rolling_std_24h', 'DEWP_rolling_mean_48h', 'DEWP_rolling_std_48h', 'PRES_rolling_mean_6h', 'PRES_rolling_std_6h', 'PRES_rolling_mean_12h', 'PRES_rolling_std_12h', 'PRES_rolling_mean_24h', 'PRES_rolling_std_24h', 'PRES_rolling_mean_48h', 'PRES_rolling_std_48h', 'Iws_rolling_mean_6h', 'Iws_rolling_std_6h', 'Iws_rolling_mean_12h', 'Iws_rolling_std_12h', 'Iws_rolling_mean_24h', 'Iws_rolling_std_24h', 'Iws_rolling_mean_48h', 'Iws_rolling_std_48h', 'pm25_lag_24h', 'pm25_lag_48h', 'pm25_lag_72h']\n",
      "\n",
      "Preprocessor structure:\n",
      "\n",
      "Transformer: num\n",
      "Columns:\n",
      "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'hour_of_day', 'day_of_week', 'month_of_year', 'hour_of_day_sin', 'hour_of_day_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_of_year_sin', 'month_of_year_cos', 'humid_temp', 'wind_pressure', 'TEMP_rolling_mean_6h', 'TEMP_rolling_std_6h', 'TEMP_rolling_mean_12h', 'TEMP_rolling_std_12h', 'TEMP_rolling_mean_24h', 'TEMP_rolling_std_24h', 'TEMP_rolling_mean_48h', 'TEMP_rolling_std_48h', 'DEWP_rolling_mean_6h', 'DEWP_rolling_std_6h', 'DEWP_rolling_mean_12h', 'DEWP_rolling_std_12h', 'DEWP_rolling_mean_24h', 'DEWP_rolling_std_24h', 'DEWP_rolling_mean_48h', 'DEWP_rolling_std_48h', 'PRES_rolling_mean_6h', 'PRES_rolling_std_6h', 'PRES_rolling_mean_12h', 'PRES_rolling_std_12h', 'PRES_rolling_mean_24h', 'PRES_rolling_std_24h', 'PRES_rolling_mean_48h', 'PRES_rolling_std_48h', 'Iws_rolling_mean_6h', 'Iws_rolling_std_6h', 'Iws_rolling_mean_12h', 'Iws_rolling_std_12h', 'Iws_rolling_mean_24h', 'Iws_rolling_std_24h', 'Iws_rolling_mean_48h', 'Iws_rolling_std_48h', 'pm25_lag_24h', 'pm25_lag_48h', 'pm25_lag_72h']\n",
      "\n",
      "Transformer: cat\n",
      "Columns:\n",
      "['cbwd']\n",
      "\n",
      "Missing values after feature engineering:\n",
      "160\n",
      "\n",
      "Final dataframe shape: (41757, 64)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Test Initial Setup\n",
    "\n",
    "# Load and process data\n",
    "file_path = 'PRSA_data_2010.1.1-2014.12.31.csv'\n",
    "df = load_and_clean_data(file_path)\n",
    "\n",
    "# Create advanced features\n",
    "df = create_advanced_features(df)\n",
    "\n",
    "# Check the features we created\n",
    "print(\"\\nColumns in processed dataframe:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Create preprocessor and check its structure\n",
    "preprocessor = create_preprocessor()\n",
    "print(\"\\nPreprocessor structure:\")\n",
    "# Print the transformers and their associated columns\n",
    "for name, transformer, columns in preprocessor.transformers:\n",
    "    print(f\"\\nTransformer: {name}\")\n",
    "    print(\"Columns:\")\n",
    "    print(columns)\n",
    "\n",
    "# Quick check for any missing values after feature engineering\n",
    "print(\"\\nMissing values after feature engineering:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Print shape of data\n",
    "print(f\"\\nFinal dataframe shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 57\n",
      "Number of samples: 41685\n",
      "\n",
      "All required features are present in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Data Preperation \n",
    "\n",
    "# Drop rows with NaN values after feature engineering\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['pm2.5', 'date', 'No', 'year', 'month', 'day', 'hour']]\n",
    "X = df[feature_cols]\n",
    "y = df['pm2.5']\n",
    "\n",
    "# Print feature information\n",
    "print(f\"Number of features used: {len(feature_cols)}\")\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "\n",
    "# Check if all required features are present\n",
    "preprocessor = create_preprocessor()\n",
    "all_features = []\n",
    "for _, _, columns in preprocessor.transformers:\n",
    "    all_features.extend(columns)\n",
    "\n",
    "missing_features = set(all_features) - set(X.columns)\n",
    "if missing_features:\n",
    "    print(\"\\nWarning: Missing features:\")\n",
    "    print(missing_features)\n",
    "else:\n",
    "    print(\"\\nAll required features are present in the dataset.\")\n",
    "\n",
    "# Initialize cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 92.09 ± 4.36\n",
      "\n",
      "Evaluating Lasso...\n",
      "Fold 1 - Test RMSE: 72.10\n",
      "Fold 2 - Test RMSE: 66.59\n",
      "Fold 3 - Test RMSE: 60.80\n",
      "Fold 4 - Test RMSE: 65.87\n",
      "Overall Performance - RMSE: 66.34 ± 4.00\n",
      "\n",
      "Evaluating Ridge...\n",
      "Fold 1 - Test RMSE: 72.08\n",
      "Fold 2 - Test RMSE: 66.91\n",
      "Fold 3 - Test RMSE: 60.78\n",
      "Fold 4 - Test RMSE: 64.81\n",
      "Overall Performance - RMSE: 66.14 ± 4.07\n",
      "\n",
      "Evaluating RandomForest...\n",
      "Fold 1 - Test RMSE: 69.88\n",
      "Fold 2 - Test RMSE: 67.35\n",
      "Fold 3 - Test RMSE: 60.15\n",
      "Fold 4 - Test RMSE: 61.18\n",
      "Overall Performance - RMSE: 64.64 ± 4.09\n",
      "\n",
      "XGBoost Configuration:\n",
      "Base parameters: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.9, 'device': None, 'early_stopping_rounds': 50, 'enable_categorical': True, 'eval_metric': 'rmse', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.03, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 10000, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.66, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "\n",
      "Parameter grid being searched: {'regressor__max_depth': [1, 3, 10, 30], 'regressor__reg_alpha': [0.01, 1, 100], 'regressor__reg_lambda': [0.01, 1, 100]}\n",
      "\n",
      "Evaluating XGBoost...\n",
      "\n",
      "Fold 1 - XGBoost Training:\n",
      "Training set shape: (6669, 59)\n",
      "Validation set shape: (1668, 59)\n",
      "Best parameters for fold 1: {'regressor__max_depth': 3, 'regressor__reg_alpha': 100, 'regressor__reg_lambda': 100}\n",
      "Best validation RMSE: 76.57\n",
      "Fold 1 - Test RMSE: 67.48\n",
      "\n",
      "Fold 2 - XGBoost Training:\n",
      "Training set shape: (13339, 59)\n",
      "Validation set shape: (3335, 59)\n",
      "Best parameters for fold 2: {'regressor__max_depth': 3, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 100}\n",
      "Best validation RMSE: 70.08\n",
      "Fold 2 - Test RMSE: 62.58\n",
      "\n",
      "Fold 3 - XGBoost Training:\n",
      "Training set shape: (20008, 59)\n",
      "Validation set shape: (5003, 59)\n",
      "Best parameters for fold 3: {'regressor__max_depth': 3, 'regressor__reg_alpha': 0.01, 'regressor__reg_lambda': 100}\n",
      "Best validation RMSE: 66.84\n",
      "Fold 3 - Test RMSE: 57.34\n",
      "\n",
      "Fold 4 - XGBoost Training:\n",
      "Training set shape: (26678, 59)\n",
      "Validation set shape: (6670, 59)\n",
      "Best parameters for fold 4: {'regressor__max_depth': 3, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 100}\n",
      "Best validation RMSE: 51.07\n",
      "Fold 4 - Test RMSE: 57.10\n",
      "Overall Performance - RMSE: 61.13 ± 4.27\n",
      "\n",
      "Summary of best parameters across folds:\n",
      "Fold 1: {'regressor__max_depth': 3, 'regressor__reg_alpha': 100, 'regressor__reg_lambda': 100}\n",
      "Fold 2: {'regressor__max_depth': 3, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 100}\n",
      "Fold 3: {'regressor__max_depth': 3, 'regressor__reg_alpha': 0.01, 'regressor__reg_lambda': 100}\n",
      "Fold 4: {'regressor__max_depth': 3, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 100}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Time Series Model Evaluation Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "import shap\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Initialize models with parameter grids based on professor's requirements\n",
    "models = {\n",
    "    'Lasso': (\n",
    "        Lasso(random_state=42),\n",
    "        {\n",
    "            'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "            'regressor__max_iter': [10000],\n",
    "        }\n",
    "    ),\n",
    "    'Ridge': (\n",
    "        Ridge(random_state=42),\n",
    "        {\n",
    "            'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        }\n",
    "    ),\n",
    "    'RandomForest': (\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=100),\n",
    "        {\n",
    "            'regressor__max_depth': [1, 3, 10, 30, 100, 300],\n",
    "            'regressor__min_samples_split': [2, 3, 10, 30],\n",
    "            'regressor__max_features': [3, 5, 7, None]\n",
    "        }\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=10000,\n",
    "            early_stopping_rounds=50,\n",
    "            enable_categorical=True,\n",
    "            colsample_bytree=0.9,\n",
    "            subsample=0.66,\n",
    "            missing=np.nan,\n",
    "            eval_metric='rmse'\n",
    "        ),\n",
    "        {\n",
    "            'regressor__max_depth': [1, 3, 10, 30],\n",
    "            'regressor__reg_alpha': [0.01, 1, 100],\n",
    "            'regressor__reg_lambda': [0.01, 1, 100]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "def get_train_val_split(X_other, y_other):\n",
    "    \"\"\"Split other data into train and validation maintaining temporal order\"\"\"\n",
    "    n = len(X_other)\n",
    "    train_size = int(0.8 * n)  # Use 80% for training, 20% for validation\n",
    "    \n",
    "    X_train = X_other.iloc[:train_size]\n",
    "    X_val = X_other.iloc[train_size:]\n",
    "    y_train = y_other.iloc[:train_size]\n",
    "    y_val = y_other.iloc[train_size:]\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def evaluate_model(X, y, model_name, model, param_grid, n_splits=4):\n",
    "    \"\"\"Evaluate a single model using time series cross-validation\"\"\"\n",
    "    if model_name == 'XGBoost':\n",
    "        X = X.copy()\n",
    "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_columns:\n",
    "            X[col] = X[col].astype('category')\n",
    "            \n",
    "        print(\"\\nXGBoost Configuration:\")\n",
    "        print(\"Base parameters:\", model.get_params())\n",
    "        print(\"\\nParameter grid being searched:\", param_grid)\n",
    "    \n",
    "    # Stage 1: TimeSeriesSplit with 4 folds\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    test_scores = []\n",
    "    best_params_list = []\n",
    "    all_predictions = []\n",
    "    all_true_values = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    for fold, (other_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        # Stage 1: Split into other and test sets\n",
    "        X_other, X_test = X.iloc[other_idx], X.iloc[test_idx]\n",
    "        y_other, y_test = y.iloc[other_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Stage 2: Split other into train and validation\n",
    "        X_train, X_val, y_train, y_val = get_train_val_split(X_other, y_other)\n",
    "        \n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        # Grid search without cross-validation\n",
    "        if model_name == 'XGBoost':\n",
    "            preprocessor = create_preprocessor()\n",
    "            preprocessor.fit(pd.concat([X_train, X_val, X_test]))\n",
    "            \n",
    "            X_train_processed = preprocessor.transform(X_train)\n",
    "            X_val_processed = preprocessor.transform(X_val)\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "            \n",
    "            print(f\"\\nFold {fold} - XGBoost Training:\")\n",
    "            print(f\"Training set shape: {X_train_processed.shape}\")\n",
    "            print(f\"Validation set shape: {X_val_processed.shape}\")\n",
    "            \n",
    "            for params in ParameterGrid(param_grid):\n",
    "                xgb_model = clone(model)\n",
    "                xgb_model.set_params(**{k.replace('regressor__', ''): v for k, v in params.items()})\n",
    "                \n",
    "                xgb_model.fit(\n",
    "                    X_train_processed, y_train,\n",
    "                    eval_set=[(X_val_processed, y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                val_pred = xgb_model.predict(X_val_processed)\n",
    "                val_score = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "                \n",
    "                if val_score < best_score:\n",
    "                    best_score = val_score\n",
    "                    best_params = params\n",
    "                    best_model = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('regressor', xgb_model)\n",
    "                    ])\n",
    "            \n",
    "            print(f\"Best parameters for fold {fold}:\", best_params)\n",
    "            print(f\"Best validation RMSE: {best_score:.2f}\")\n",
    "            \n",
    "        else:\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                pipeline = Pipeline([\n",
    "                    ('preprocessor', create_preprocessor()),\n",
    "                    ('regressor', clone(model))\n",
    "                ])\n",
    "                pipeline.set_params(**params)\n",
    "                \n",
    "                pipeline.fit(X_train, y_train)\n",
    "                val_pred = pipeline.predict(X_val)\n",
    "                val_score = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "                \n",
    "                if val_score < best_score:\n",
    "                    best_score = val_score\n",
    "                    best_params = params\n",
    "                    best_model = pipeline\n",
    "        \n",
    "        # Make predictions on test set and store score\n",
    "        test_pred = best_model.predict(X_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        \n",
    "        test_scores.append(test_rmse)\n",
    "        best_params_list.append(best_params)\n",
    "        all_predictions.extend(test_pred)\n",
    "        all_true_values.extend(y_test)\n",
    "        \n",
    "        print(f\"Fold {fold} - Test RMSE: {test_rmse:.2f}\")\n",
    "    \n",
    "    # Calculate mean and std of test scores\n",
    "    mean_score = np.mean(test_scores)\n",
    "    std_score = np.std(test_scores)\n",
    "    print(f\"Overall Performance - RMSE: {mean_score:.2f} ± {std_score:.2f}\")\n",
    "    \n",
    "    if model_name == 'XGBoost':\n",
    "        print(\"\\nSummary of best parameters across folds:\")\n",
    "        for fold, params in enumerate(best_params_list, 1):\n",
    "            print(f\"Fold {fold}: {params}\")\n",
    "    \n",
    "    return {\n",
    "        'test_scores': test_scores,\n",
    "        'best_params': best_params_list,\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'true_values': np.array(all_true_values),\n",
    "        'final_model': best_model,\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score\n",
    "    }\n",
    "\n",
    "# Calculate baseline scores\n",
    "def calculate_baseline(y, n_splits=4):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    baseline_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(y):\n",
    "        y_train = y.iloc[train_idx]\n",
    "        y_test = y.iloc[test_idx]\n",
    "        # Use mean of training set as prediction\n",
    "        baseline_pred = [y_train.mean()] * len(y_test)\n",
    "        baseline_scores.append(np.sqrt(mean_squared_error(y_test, baseline_pred)))\n",
    "    \n",
    "    return np.mean(baseline_scores), np.std(baseline_scores)\n",
    "\n",
    "# Run evaluation\n",
    "baseline_mean, baseline_std = calculate_baseline(y)\n",
    "print(f\"Baseline RMSE: {baseline_mean:.2f} ± {baseline_std:.2f}\")\n",
    "\n",
    "results = {}\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    results[model_name] = evaluate_model(\n",
    "        X, y, model_name, model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_31eda th {\n",
       "  text-align: left;\n",
       "  background-color: #f2f2f2;\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_31eda td {\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_31eda_row0_col0, #T_31eda_row0_col1, #T_31eda_row0_col2, #T_31eda_row1_col0, #T_31eda_row1_col1, #T_31eda_row1_col2, #T_31eda_row2_col0, #T_31eda_row2_col1, #T_31eda_row2_col2, #T_31eda_row3_col0, #T_31eda_row3_col1, #T_31eda_row3_col2, #T_31eda_row4_col0, #T_31eda_row4_col1, #T_31eda_row4_col2, #T_31eda_row5_col0, #T_31eda_row5_col1, #T_31eda_row5_col2, #T_31eda_row6_col0, #T_31eda_row6_col1, #T_31eda_row6_col2, #T_31eda_row7_col0, #T_31eda_row7_col1, #T_31eda_row7_col2, #T_31eda_row8_col0, #T_31eda_row8_col1, #T_31eda_row8_col2, #T_31eda_row9_col0, #T_31eda_row9_col1, #T_31eda_row9_col2, #T_31eda_row10_col0, #T_31eda_row10_col1, #T_31eda_row10_col2, #T_31eda_row11_col0, #T_31eda_row11_col1, #T_31eda_row11_col2, #T_31eda_row12_col0, #T_31eda_row12_col1, #T_31eda_row12_col2, #T_31eda_row13_col0, #T_31eda_row13_col1, #T_31eda_row13_col2, #T_31eda_row14_col0, #T_31eda_row14_col1, #T_31eda_row14_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_31eda\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_31eda_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_31eda_level0_col1\" class=\"col_heading level0 col1\" >Hyperparameter</th>\n",
       "      <th id=\"T_31eda_level0_col2\" class=\"col_heading level0 col2\" >Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row0_col0\" class=\"data row0 col0\" >Lasso</td>\n",
       "      <td id=\"T_31eda_row0_col1\" class=\"data row0 col1\" >alpha</td>\n",
       "      <td id=\"T_31eda_row0_col2\" class=\"data row0 col2\" >[0.001, 0.01, 0.1, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row1_col0\" class=\"data row1 col0\" >Lasso</td>\n",
       "      <td id=\"T_31eda_row1_col1\" class=\"data row1 col1\" >max_iter</td>\n",
       "      <td id=\"T_31eda_row1_col2\" class=\"data row1 col2\" >[10000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row2_col0\" class=\"data row2 col0\" >Ridge</td>\n",
       "      <td id=\"T_31eda_row2_col1\" class=\"data row2 col1\" >alpha</td>\n",
       "      <td id=\"T_31eda_row2_col2\" class=\"data row2 col2\" >[0.001, 0.01, 0.1, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row3_col0\" class=\"data row3 col0\" >Random Forest</td>\n",
       "      <td id=\"T_31eda_row3_col1\" class=\"data row3 col1\" >n_estimators</td>\n",
       "      <td id=\"T_31eda_row3_col2\" class=\"data row3 col2\" >100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row4_col0\" class=\"data row4 col0\" >Random Forest</td>\n",
       "      <td id=\"T_31eda_row4_col1\" class=\"data row4 col1\" >max_depth</td>\n",
       "      <td id=\"T_31eda_row4_col2\" class=\"data row4 col2\" >[1, 3, 10, 30, 100, 300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row5_col0\" class=\"data row5 col0\" >Random Forest</td>\n",
       "      <td id=\"T_31eda_row5_col1\" class=\"data row5 col1\" >min_samples_split</td>\n",
       "      <td id=\"T_31eda_row5_col2\" class=\"data row5 col2\" >[2, 3, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row6_col0\" class=\"data row6 col0\" >Random Forest</td>\n",
       "      <td id=\"T_31eda_row6_col1\" class=\"data row6 col1\" >max_features</td>\n",
       "      <td id=\"T_31eda_row6_col2\" class=\"data row6 col2\" >[3, 5, 7, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row7_col0\" class=\"data row7 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row7_col1\" class=\"data row7 col1\" >learning_rate</td>\n",
       "      <td id=\"T_31eda_row7_col2\" class=\"data row7 col2\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row8_col0\" class=\"data row8 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row8_col1\" class=\"data row8 col1\" >n_estimators</td>\n",
       "      <td id=\"T_31eda_row8_col2\" class=\"data row8 col2\" >10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row9_col0\" class=\"data row9 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row9_col1\" class=\"data row9 col1\" >early_stopping_rounds</td>\n",
       "      <td id=\"T_31eda_row9_col2\" class=\"data row9 col2\" >50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row10_col0\" class=\"data row10 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row10_col1\" class=\"data row10 col1\" >colsample_bytree</td>\n",
       "      <td id=\"T_31eda_row10_col2\" class=\"data row10 col2\" >0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row11_col0\" class=\"data row11 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row11_col1\" class=\"data row11 col1\" >subsample</td>\n",
       "      <td id=\"T_31eda_row11_col2\" class=\"data row11 col2\" >0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row12_col0\" class=\"data row12 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row12_col1\" class=\"data row12 col1\" >max_depth</td>\n",
       "      <td id=\"T_31eda_row12_col2\" class=\"data row12 col2\" >[1, 3, 10, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row13_col0\" class=\"data row13 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row13_col1\" class=\"data row13 col1\" >reg_alpha</td>\n",
       "      <td id=\"T_31eda_row13_col2\" class=\"data row13 col2\" >[0.01, 1, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31eda_row14_col0\" class=\"data row14 col0\" >XGBoost</td>\n",
       "      <td id=\"T_31eda_row14_col1\" class=\"data row14 col1\" >reg_lambda</td>\n",
       "      <td id=\"T_31eda_row14_col2\" class=\"data row14 col2\" >[0.01, 1, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a502450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model_summary_table():\n",
    "    summary_data = {\n",
    "        'Model': [],\n",
    "        'Hyperparameter': [],\n",
    "        'Values': []\n",
    "    }\n",
    "    \n",
    "    # Lasso\n",
    "    summary_data['Model'].append('Lasso')\n",
    "    summary_data['Hyperparameter'].append('alpha')\n",
    "    summary_data['Values'].append('[0.001, 0.01, 0.1, 1.0]')\n",
    "    summary_data['Model'].append('Lasso')\n",
    "    summary_data['Hyperparameter'].append('max_iter')\n",
    "    summary_data['Values'].append('[10000]')\n",
    "    \n",
    "    # Ridge\n",
    "    summary_data['Model'].append('Ridge')\n",
    "    summary_data['Hyperparameter'].append('alpha')\n",
    "    summary_data['Values'].append('[0.001, 0.01, 0.1, 1.0]')\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_params = ['n_estimators', 'max_depth', 'min_samples_split', 'max_features']\n",
    "    rf_values = ['100', '[1, 3, 10, 30, 100, 300]', '[2, 3, 10, 30]', '[3, 5, 7, None]']\n",
    "    for param, value in zip(rf_params, rf_values):\n",
    "        summary_data['Model'].append('Random Forest')\n",
    "        summary_data['Hyperparameter'].append(param)\n",
    "        summary_data['Values'].append(value)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_base_params = ['learning_rate', 'n_estimators', 'early_stopping_rounds', 'colsample_bytree', 'subsample']\n",
    "    xgb_base_values = ['0.03', '10000', '50', '0.9', '0.66']\n",
    "    for param, value in zip(xgb_base_params, xgb_base_values):\n",
    "        summary_data['Model'].append('XGBoost')\n",
    "        summary_data['Hyperparameter'].append(param)\n",
    "        summary_data['Values'].append(value)\n",
    "        \n",
    "    xgb_tuned_params = ['max_depth', 'reg_alpha', 'reg_lambda']\n",
    "    xgb_tuned_values = ['[1, 3, 10, 30]', '[0.01, 1, 100]', '[0.01, 1, 100]']\n",
    "    for param, value in zip(xgb_tuned_params, xgb_tuned_values):\n",
    "        summary_data['Model'].append('XGBoost')\n",
    "        summary_data['Hyperparameter'].append(param)\n",
    "        summary_data['Values'].append(value)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display table with styling\n",
    "    return (summary_df.style\n",
    "            .set_properties(**{'text-align': 'left'})\n",
    "            .set_table_styles([\n",
    "                {'selector': 'th', 'props': [('text-align', 'left'),\n",
    "                                           ('background-color', '#f2f2f2'),\n",
    "                                           ('padding', '8px')]},\n",
    "                {'selector': 'td', 'props': [('padding', '8px')]}\n",
    "            ])\n",
    "            .hide(axis='index'))\n",
    "\n",
    "# Display the summary table\n",
    "model_summary = create_model_summary_table()\n",
    "display(model_summary)\n",
    "\n",
    "# Save the table as a high-resolution figure\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "ax = plt.subplot(111, frame_on=False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "\n",
    "# Create table\n",
    "table = plt.table(cellText=pd.DataFrame(model_summary.data).values,\n",
    "                 colLabels=['Model', 'Hyperparameter', 'Values'],\n",
    "                 cellLoc='left',\n",
    "                 loc='center',\n",
    "                 colWidths=[0.2, 0.3, 0.5])\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('model_hyperparameters_summary.png', \n",
    "            dpi=300,\n",
    "            bbox_inches='tight',\n",
    "            format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Summary\n",
      "====================================================================================================\n",
      "       Model         RMSE Improvement                                    Best Parameters\n",
      "       Lasso 66.34 ± 4.00       28.0%                          alpha=1.0, max_iter=10000\n",
      "       Ridge 66.14 ± 4.07       28.2%                                          alpha=1.0\n",
      "RandomForest 64.64 ± 4.09       29.8% max_depth=100, max_features=7, min_samples_split=3\n",
      "     XGBoost 61.13 ± 4.27       33.6%           max_depth=3, reg_alpha=1, reg_lambda=100\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Enhanced Results Analysis and Model Interpretation\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def create_improved_force_plots(explainer, shap_values, X_processed, feature_names, indices=[0, 100, 200]):\n",
    "    \"\"\"Create improved SHAP force plots with enhanced readability and captions\"\"\"\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    for idx in indices:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        shap.force_plot(\n",
    "            explainer.expected_value,\n",
    "            shap_values[idx],\n",
    "            X_processed[idx],\n",
    "            feature_names=feature_names,\n",
    "            matplotlib=True,\n",
    "            show=False,\n",
    "            text_rotation=45,\n",
    "            contribution_threshold=0.05\n",
    "        )\n",
    "        \n",
    "        plt.title(f'SHAP Force Plot: Feature Contributions for Sample {idx}', pad=20, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Add detailed caption\n",
    "        caption = (f'Feature contribution analysis for sample {idx}. Positive values (red) '\n",
    "                  'increase the prediction, while negative values (blue) decrease it.')\n",
    "        plt.figtext(0.5, 0.02, caption, ha='center', fontsize=10, wrap=True)\n",
    "        \n",
    "        plt.savefig(f'force_plot_{idx}.png', dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "        plt.close()\n",
    "def analyze_results(results, baseline_mean, baseline_std):\n",
    "    \"\"\"\n",
    "    Enhanced analysis function with improved visualizations and SHAP analysis\n",
    "    \"\"\"\n",
    "    # 1. Performance Comparison Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models = list(results.keys())\n",
    "    means = [results[m]['mean_score'] for m in models]\n",
    "    stds = [results[m]['std_score'] for m in models]\n",
    "    \n",
    "    plt.errorbar(range(len(models)), means, yerr=stds, fmt='o', capsize=5,\n",
    "                markersize=8, elinewidth=2, capthick=2)\n",
    "    plt.axhline(y=baseline_mean, color='r', linestyle='--', \n",
    "                label=f'Baseline ({baseline_mean:.2f} ± {baseline_std:.2f})')\n",
    "    plt.xticks(range(len(models)), models, rotation=45)\n",
    "    plt.xlabel('Model Type')  # Added x-axis label\n",
    "    plt.ylabel('RMSE (μg/m³)')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Best Model Analysis (XGBoost was best)\n",
    "    best_model_results = results['XGBoost']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(best_model_results['true_values'], \n",
    "               best_model_results['predictions'], \n",
    "               alpha=0.5, color='blue', s=20, \n",
    "               label='Predictions')\n",
    "    \n",
    "    max_val = max(max(best_model_results['true_values']), \n",
    "                  max(best_model_results['predictions']))\n",
    "    min_val = min(min(best_model_results['true_values']), \n",
    "                  min(best_model_results['predictions']))\n",
    "    \n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "             lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    z = np.polyfit(best_model_results['true_values'], \n",
    "                   best_model_results['predictions'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(best_model_results['true_values'], \n",
    "             p(best_model_results['true_values']), \n",
    "             \"g-\", alpha=0.8, \n",
    "             label=f'Trend Line (y = {z[0]:.2f}x + {z[1]:.2f})')\n",
    "    \n",
    "    r2 = r2_score(best_model_results['true_values'], \n",
    "                  best_model_results['predictions'])\n",
    "    plt.text(0.05, 0.95, f'R² = {r2:.3f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.xlabel('Actual PM2.5 (μg/m³)')\n",
    "    plt.ylabel('Predicted PM2.5 (μg/m³)')\n",
    "    plt.title('XGBoost - Actual vs Predicted PM2.5 Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Error Distribution\n",
    "    errors = best_model_results['predictions'] - best_model_results['true_values']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=50, alpha=0.75, color='blue')\n",
    "    plt.axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "    plt.title('Distribution of XGBoost Prediction Errors')\n",
    "    plt.xlabel('Prediction Error (μg/m³)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Error Analysis by PM2.5 Level\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(best_model_results['true_values'], \n",
    "               errors, \n",
    "               alpha=0.5, s=20)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', label='Zero Error')\n",
    "    plt.xlabel('Actual PM2.5 (μg/m³)')\n",
    "    plt.ylabel('Prediction Error (μg/m³)')\n",
    "    plt.title('Error Analysis by PM2.5 Level')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 5. SHAP Analysis\n",
    "    if hasattr(best_model_results['final_model']['regressor'], 'feature_importances_'):\n",
    "        preprocessor = best_model_results['final_model']['preprocessor']\n",
    "        feature_names = (preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +\n",
    "                        preprocessor.named_transformers_['cat'].get_feature_names_out().tolist())\n",
    "        \n",
    "        explainer = shap.TreeExplainer(best_model_results['final_model']['regressor'])\n",
    "        X_processed = best_model_results['final_model']['preprocessor'].transform(X.iloc[:1000])\n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(\n",
    "            shap_values, \n",
    "            X_processed,\n",
    "            feature_names=feature_names,\n",
    "            max_display=10,\n",
    "            plot_size=(12, 8),\n",
    "            show=False\n",
    "        )\n",
    "        plt.title('Top 10 Features by SHAP Value Impact', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        create_improved_force_plots(explainer, shap_values, X_processed, feature_names)\n",
    "\n",
    "    # Print model comparison results\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'RMSE': [f\"{results[m]['mean_score']:.2f} ± {results[m]['std_score']:.2f}\" for m in models],\n",
    "        'Improvement': [f\"{((baseline_mean - results[m]['mean_score']) / baseline_mean * 100):.1f}%\" for m in models],\n",
    "        'Best Parameters': [\n",
    "            ', '.join(f\"{k.split('__')[-1]}={v}\" for k, v in results[m]['best_params'][-1].items())\n",
    "            for m in models\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    print(\"\\nModel Performance Summary\")\n",
    "    print(\"=\" * 100)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate results\n",
    "summary_df = analyze_results(results, baseline_mean, baseline_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_summary_tables():\n",
    "    \"\"\"Create and format analysis summary tables with proper styling\"\"\"\n",
    "    # Table 1: Model Interpretability Steps\n",
    "    steps_df = pd.DataFrame({\n",
    "        'Step': ['1', '2', '3', '4', '5'],\n",
    "        'Analysis Type': [\n",
    "            'Global Feature Importance',\n",
    "            'Local Feature Impact',\n",
    "            'Error Analysis',\n",
    "            'Model Performance Visualization',\n",
    "            'Cross-validation'\n",
    "        ],\n",
    "        'Description': [\n",
    "            'SHAP analysis with feature importance visualization',\n",
    "            'Individual prediction explanations with force plots',\n",
    "            'Error distribution and PM2.5 level dependency analysis',\n",
    "            'Actual vs Predicted with residual analysis',\n",
    "            'Temporal stability assessment with cross-validation'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Table 2: Key Findings with actual results and proper formatting\n",
    "    findings_df = pd.DataFrame({\n",
    "        'Category': [\n",
    "            'Model Performance',\n",
    "            'Model Performance',\n",
    "            'Model Performance',\n",
    "            'Model Parameters',\n",
    "            'Model Parameters',\n",
    "            'Model Behavior',\n",
    "            'Model Behavior'\n",
    "        ],\n",
    "        'Finding': [\n",
    "            f\"XGBoost achieved best performance (RMSE: {61.13:.2f} ± {4.27:.2f})\",\n",
    "            f\"Significant improvement over baseline ({92.09:.2f} ± {4.36:.2f})\",\n",
    "            \"All models exceeded baseline by >28%\",\n",
    "            \"XGBoost optimal with max_depth=3\",\n",
    "            \"High L2 regularization (reg_lambda=100) preferred\",\n",
    "            \"Enhanced accuracy in later time periods\",\n",
    "            \"Consistent cross-validation performance\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Apply consistent styling with proper captions\n",
    "    for df, name, caption in [(steps_df, 'steps', 'Model Interpretation Steps and Methodology'),\n",
    "                             (findings_df, 'findings', 'Key Model Performance Findings')]:\n",
    "        plt.figure(figsize=(12, 6), dpi=300)\n",
    "        ax = plt.subplot(111, frame_on=False)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "        table = plt.table(\n",
    "            cellText=df.values,\n",
    "            colLabels=df.columns,\n",
    "            cellLoc='left',\n",
    "            loc='center',\n",
    "            colWidths=[0.1, 0.3, 0.6] if name == 'steps' else [0.3, 0.7]\n",
    "        )\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        plt.title(caption, pad=20)\n",
    "        plt.savefig(f'model_analysis_{name}.png',\n",
    "                   dpi=300,\n",
    "                   bbox_inches='tight',\n",
    "                   format='png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xgboost_importance(results):\n",
    "    \"\"\"Create XGBoost feature importance visualizations with visible captions\"\"\"\n",
    "    best_model = results['XGBoost']['final_model']\n",
    "    xgb_model = best_model['regressor']\n",
    "    booster = xgb_model.get_booster()\n",
    "    \n",
    "    # Get feature names with proper formatting\n",
    "    preprocessor = best_model['preprocessor']\n",
    "    actual_feature_names = np.concatenate([\n",
    "        preprocessor.named_transformers_['num'].get_feature_names_out(),\n",
    "        preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
    "    ])\n",
    "    \n",
    "    name_map = {f'f{i}': name for i, name in enumerate(actual_feature_names)}\n",
    "    \n",
    "    def plot_importance(importance_dict, metric_name):\n",
    "        # Convert and sort importance scores\n",
    "        renamed_importance = {name_map[k]: v for k, v in importance_dict.items()}\n",
    "        sorted_items = sorted(renamed_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_features = [x[0] for x in sorted_items[:10]]\n",
    "        top_scores = [x[1] for x in sorted_items[:10]]\n",
    "        \n",
    "        # Create figure with extra space for caption\n",
    "        plt.figure(figsize=(12, 7))  # Increased height for caption\n",
    "        \n",
    "        # Create main axis for the plot, leaving space at bottom for caption\n",
    "        ax = plt.gca()\n",
    "        ax.set_position([0.1, 0.15, 0.85, 0.75])  # [left, bottom, width, height]\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.barh(range(len(top_features)), top_scores, color='skyblue')\n",
    "        \n",
    "        # Format feature names for readability\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels([name.replace('_', ' ').title() for name in top_features])\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                   f'{width:.2f}',\n",
    "                   ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        # Add labels and title\n",
    "        ax.set_xlabel(f'Importance Score ({metric_name})')\n",
    "        ax.set_title(f'Top 10 Most Important Features in PM2.5 Prediction ({metric_name})')\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add caption at the bottom\n",
    "        caption_text = {\n",
    "            'weight': 'Number of times a feature appears in the trees.',\n",
    "            'gain': 'Average gain of splits which use this feature.',\n",
    "            'cover': 'Average coverage of splits which use this feature.',\n",
    "            'total_gain': 'Total gain of splits which use this feature.',\n",
    "            'total_cover': 'Total coverage of splits which use this feature.'\n",
    "        }\n",
    "        \n",
    "        plt.figtext(0.1, 0.02, \n",
    "                   f\"Caption: {caption_text[metric_name]} Higher scores indicate stronger influence on predictions.\",\n",
    "                   wrap=True, \n",
    "                   ha='left', \n",
    "                   va='bottom',\n",
    "                   fontsize=10)\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(f'xgboost_importance_{metric_name}.png',\n",
    "                   dpi=300,\n",
    "                   bbox_inches='tight',\n",
    "                   pad_inches=0.2)  # Added padding to prevent caption cutoff\n",
    "        plt.close()\n",
    "\n",
    "    # Generate plots for all importance metrics\n",
    "    for metric in ['weight', 'gain', 'cover', 'total_gain', 'total_cover']:\n",
    "        importance = booster.get_score(importance_type=metric)\n",
    "        plot_importance(importance, metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
